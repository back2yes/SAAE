\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}	% for \begin{align}
\usepackage{graphicx}	% for \includegraphics{filename}
\usepackage{subcaption}	% for \begin{subfigure}[t]{0.5\textwidth}

\newcommand{\bb}[1]{\boldsymbol{#1}}

\title{Supplementary of Adversarial Zero-Shot Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Shihui Li, Yu-Hsiang Lin, Kangyan Zhou
		%\thanks{Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}
		\\
	Language Technologies Institute\\
	Carnegie Mellon University\\
	Pittsburgh, PA 15213 \\
	\texttt{\{shihuil,yuhsianl,kangyanz\}@andrew.cmu.edu} \\
	%% examples of more authors
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

%\begin{abstract}

%	This is a 4-page draft.

%\end{abstract}



% ----------------------------------------------------
% ----------------------------------------------------

\section{Adversarial autoencoder with the prior as the regularizer}

	Given an input image representation $\bb{x}^c \in R^{d_x}$, the encoder encodes it into $\bb{h}^c \in R^{d_h}$ by
	\begin{align}
		\bb{h}^c = \sigma(W^e \bb{x}^c + \bb{b}^e),
	\end{align}
	where $W^e \in R^{d_h \times d_x}$, $\bb{b}^e \in R^{d_h}$, and $\sigma(\cdot)$ is the sigmoid function,
	\begin{align}
		\sigma(s) = \frac{1}{1 + e^{-s}},
	\end{align}
	applied element-wisely. Note that the hidden representation space $R^{d_h}$ is also the space of the text representation; that is, $\bb{t}^c \in R^{d_h}$. The decoder computes
	\begin{align}
		\tilde{\bb{x}}^c = \sigma(W^d \bb{h}^c + \bb{b}^d),
	\end{align}
	where $W^d \in R^{d_x \times d_h}$ and $\bb{b}^d \in R^{d_x}$, such that the reconstruction cross entropy,
	\begin{align}
		S = -\bb{x}^c \odot \log \tilde{\bb{x}}^c,
	\end{align}
	where the log being applied element-wisely, is to be minimized.
	
	The generator of the adversarial net is the encoder of the autoencoder; that is,
	\begin{align}
		G(\bb{x}^c) = \sigma(W^e \bb{x}^c + \bb{b}^e) = \bb{h}^c.
	\end{align}
	The positive samples are drawn from the Gaussian prior,
	\begin{align}
		\bb{z} \sim \mathcal{N}(0, \lambda^2).
	\end{align}
	where $\bb{z} \in R^{d_h}$.	We use a fully connected neural network with a single hidden layer as the discriminator. Given an input $\bb{z} \in R^{d_h}$, it computes
	\begin{align}
		\bb{z} \sim \mathcal{N}(0, \lambda^2).
	\end{align}



% ----------------------------------------------------
% ----------------------------------------------------

%\subsubsection*{Acknowledgments}

%Use unnumbered third level headings for the acknowledgments. All acknowledgments go at the end of the paper. Do not include acknowledgments in the anonymized submission, only in the final paper.



% ----------------------------------------------------
% ----------------------------------------------------

%\bibliographystyle{plain}
%\bibliography{MachineLearning}

\end{document}
