\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}	% for \begin{align}
\usepackage{graphicx}	% for \includegraphics{filename}
\usepackage{subcaption}	% for \begin{subfigure}[t]{0.5\textwidth}

\newcommand{\bb}[1]{\boldsymbol{#1}}

\title{Semantic Adversarial Autoencoder for Zero-Shot Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Shihui Li, Yu-Hsiang Lin, Kangyan Zhou
		%\thanks{Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}
		\\
	Language Technologies Institute\\
	Carnegie Mellon University\\
	Pittsburgh, PA 15213 \\
	\texttt{\{shihuil,yuhsianl,kangyanz\}@andrew.cmu.edu} \\
	%% examples of more authors
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}

In zero-shot learning tasks, how to classify the data in the unseen classes in one domain by leveraging the information from the semantic domain is a main challenge. We propose a novel model, the semantic adversarial autoencoder, to address this challenge. By injecting the global information of the distribution of the semantic representation, the model acquires better generalization ability. We perform experiments on the AwA and CUB datasets, and show that our model outperforms the semantic autoencoder in the generalized zero-shot learning tasks.

\end{abstract}

% ----------------------------------------------------
% ----------------------------------------------------

\section{Introduction}

Zero-shot learning (ZSL) has been an active research topic in the field of image classification. It simulates how people learn things: when people see an unfamiliar object, they use existing knowledge and try to evaluate the new object. Ideally, a large-scale image classification system should be able to recognize novel categories based on its previous training experience. One of the main challenges for object recognition is the lack of sufficient annotations for all possible concepts. This problem becomes even more severe when we target at the task of fine-grained classification because the annotation is more expensive and the number of fine-grained classes is huge. Realizing this limitation, researchers resort to additional information, for example, textual or attributive description, to solve this problem.  

On the other hand, autoencoder has shown its great express power to present complicate distributions such human faces, natural sceneries, and natural language. It is able to convert complicated real world data distributions, e.g. text features, to very well-formed low dimension. Many studies have shown that after applying autoencoder techniques, the learned latent feature space often has clear semantic meanings. For example, in \cite{makhzani2015adversarial}, all the digits can be well embedded into a low-dimensional manifold such that similar digits will have smaller distance within that manifold.

The fundamental problem of ZSL is to extract semantically meaningful feature embeddings that could bridge the gap between similar image/text features and fine-grained classes. Autoencoder becomes an ideal option for this task because it can automatically learn representative features. In \cite{kodirov2017semantic}, the authors present a novel solution to zero-shot learning, the Semantic Autoencoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing zero-learning shot models, and the decoder should be able to reconstruct the original visual feature. Their results show that under this framework, the learned projection function from the seen classes is able to generalize better to the new unseen classes. One shortcoming of this model is that the encoder tends to learn similar features across multiple classes, which is disappointing for the task of fine-grained classification. 

Inspired by these models, we propose the model of Semantic adversarial autoencoder (SAAE). In this model, we have an encoder that projects a visual feature vector into the semantic space and a decoder that decodes and reconstructs the original visual features. Besides, we incorporates a discriminator as in \cite{makhzani2015adversarial} to ensure that generating from any part of prior space results in meaningful samples. Two types of models that use different priors are investigated. In one type of the model, the prior is the Gaussian noise prior, and in the other type the semantic representation itself is used as the prior.

% In this project we plan to explore how to develop an effective transfer learning algorithm, with focus on zero-shot learning and its combination with several popular deep learning methods, such as convolutional neural network, attention models, and Adversarial training, to detect unseen objects only with text description. \\
% Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. State-of-the-art methods for zero-shot recognition formulate learning as a embedding problem of images and side information like text \cite{Elhoseiny2013}. We propose a model that can classify unseen categories from their textual description. Inspired by the fact that there is little supervision on text data, certain noise-cancelling techniques could be applied to improve the performance of our model.\\




% ----------------------------------------------------
% --------------------------------------------------


\section{Related Works}    
Considering that fine-grained categories might share some common knowledge, a common approach is to seek for an intermediate semantic representation that could connect seen and unseen classes. Human specified attributes are first explored to represent the discriminative properties shared among both seen and unseen categories in zero-shot learning \cite{torresani2010efficient, lampert2009learning}. One limitation of this method is that the creation of attributes still relies on human labors, making it difficult to scale up to meet large scale needs. \par 

%Other forms of semantic representations have been explored since then. \cite{torresani2010efficient} employs the idea of using object detectors as the basic representations of images and the semantic concept is trained by using the entire image. \cite{mensink2014costa} attempts to capture the semantic relevance of two concepts by the co-occurrence of words.  In \cite{mensink2014costa, romera2015embarrassingly}, the authors assume that the classifier for an unseen class can be presented as a linear combination of seen class classifiers.

Apart from handcrafting attributes, another scheme is to directly use the online textual document as the additional information source. \cite{Elhoseiny2013} is one of the first works to use Wiki documents as text attributes. \cite{Ba_2015_ICCV} proposed a model that changes both the ways of extracting features from images and text domains. More specifically, the image features are extracted from the activation layer of a convolutional neural network (CNN), and then go through a linear projection layer to reduce dimensions. The input text, e.g. Wikipedia articles, is first converted to one-hot encodings of the words with their tf-idf scores, which can be viewed as attributes, and then fed into a multi-layer perceptron (MLP) to generate a deep representation, with the same dimension as the final image feature, and unique for each class. The final prediction is obtained by the dot product of the two generated features. Instead of learning an embedding space for each modalities, \cite{frome2013devise} learns joint image-word embeddings so as to embedding images and sentences into a common space.\\
	
	Another line of research is to improve the quality of the classification procedure. \cite{hariharan2012efficient} firstly proposed a SVM based classifier which takes the linear projection of both source and target domain data as the combined input. More recent works jointly project the class into an embedding space, and try to compute a compatability function $F(x, y)$ that tries to predict whether the image feature x is compatible with the embedded class feature y. Here each class is represented as a vector that contains the relevance scores of the class and a set of predefined attributes.  In \cite{akata2013label}, $F(x, y)$ takes a linear form as $F(x, y) = xWy$. \cite{xian2016latent} takes this idea one step further, where a set of $W_i$ is available for the the compatibility function $F(x, y)$, and the final prediction will choose the $W$ that can produce the highest score. This $W_i$ can be shared across different labels.  The method is called as latent embedding, since it learns a latent embedding space explicitly based on clustering. \cite{zhang2016zero} proposes a framework that generalizes deep learning embedding, label embedding, and latent embedding. \\ 
    
    Recently deep encoder-decoder has become popular for a variety of multi-modal problems. In \cite{kiros2014unifying}, they introduce an encoder-decoder pipeline that learns a multimodal joint embedding space with images and text and a novel language model for decoding distributed representations of the text semantic space. Their pipeline effectively unifies joint image-text embedding models with multimodal neural language models. \cite{kodirov2017semantic} takes a step further in multimodal model under the autoencoder paradigm. They proposed a novel zero-shot learning model based on a semantic autoencoder that uses a fast linear projection function and introduce an additional reconstruction objective function for learning a more generalisable projection function. \\ 


% 	In \cite{Elhoseiny2013}, during training, they learn the domain transfer function, $W$, from text domain to vision domain (in addition to feature extraction from text and image data). They use the approach from \cite{Kulis2011}. There is a measure of how much two feature vectors ($\bb{t}$ from text domain and $\bb{x}$ from vision domain) are similar to each other, based on $\bb{t}^{\top} W \bb{x}$. The training objective is basically that: (1) if $\bb{t}$ and $\bb{x}$ belong to the same class, their similarity should be more than a positive threshold (a margin), and (2) if $\bb{t}$ and $\bb{x}$ belong to the different classes, their similarity should be less than a negative threshold (a margin). It is trained with some regularization. (If the objective function is a direct/simple function of $\bb{t}^{\top} W \bb{x}$, it is non-convex for $\bb{t}$ and $\bb{x}$. That might be the reason that in \cite{Kulis2011} they use a more sophisticated formulation.)
	
% 	In fact, in \cite{Elhoseiny2013} during training they also need to learn the weight vectors $\bb{w}_i$, $i = 1, \dots, N_{sc}$ for ``seen classes'', but it seems that they do not describe how they learn these classifiers.
	
% 	They said that they ``learn'' the weight predictor (or the ``classifier regressor'') during training, but from their formulation it seems that as long as you have $\bb{w}_i$, then what the predictor does is that, given a new text vector $\bb{t}$, use $\bb{w}_i$ and Gaussian kernels (presumably with some fixed hyperparameters?) to compute (through an optimization problem) the predicted weight vector. So there doesn't seem to be any model parameters to learn here, but only works to do during inference time.
	
% 	During test time (inference), they give a clear constrained optimization problem to solve. What they try to optimize (in order to obtain the predicted weight vector $\bb{w}$ for an unseen-class text data $\bb{t}$) is basically:
% 		\begin{enumerate}
% 			\item Maximizing the similarity between $\bb{w}$ and $\bb{t}$ with respect to the domain transfer function $W$ (note that $W$ was trained using image feature $\bb{x}$ but here it is applied to weight vector $\bb{w}$ in image domain).
% 			\item Maximizing the log probability of having this $\bb{w}$ given $\bb{t}$ with respect to the weight predictor (and a given Gaussian prior).
% 			\item Minimizing the margin (well, it seems to be unbounded if you try to maximize the margin) of correctly labeling the seen classes all into negative side (they do not belong to the new class).
% 			\item Minimize the L2 regularizor.
% 		\end{enumerate}
% 		Their constraints are that all the seen images are classified as negative with some margin (done the slack variables, the margin, which are minimized).
	
	



% ----------------------------------------------------
% ----------------------------------------------------

% ----------------------------------------------------

% \subsection{Adversarial autoencoder}

% 	The adversarial autoencoder is a mixture of an autoencoder and a generative adversarial network. The input image $\bb{x}$ is encoded into $\bb{h}$ through encoder function $\bb{h} = f_{enc}(\bb{x})$. The reconstruction is performed by the conjugate decoder function $\tilde{\bb{x}} = f_{dec}(\bb{h})$. The autoencoder is trained to reconstruct the input image by minimizing the cross entropy loss,
% 		\begin{align}
% 			\textrm{loss} = -\bb{x}^{\top} \log \tilde{\bb{x}}.
% 		\end{align}
	
% 	The encoder also serves as the generator of the adversarial network, which generates negative samples with aggregated posterior distribution $q(\bb{h})$ that comes from the encoding distribution $q(\bb{h} | \bb{x})$ and the data distribution $p_d(\bb{x})$,
% 		\begin{align}
% 			q(\bb{h}) = \int q(\bb{h} | \bb{x}) p_d(\bb{x}) d \bb{x}.
% 		\end{align}
	
% 	A discriminator $D(\bb{z})$ gives the probability that $\bb{z}$ is a positive sample generated from a given and fixed prior distribution $r(\bb{z})$. The generator (the encoder) and discriminator are then jointly trained according to
% 		\begin{align}
% 			\min_{f_{enc}} \max_D \left\{ E_{\bb{z} \sim r(\bb{z})}[ \log D(\bb{z}) ] + E_{\bb{x} \sim p_d(\bb{x})}[ \log( 1 - D(f_{enc}(\bb{x})) ) ] \right\}.
% 		\end{align}
% 		That is, the generator (the encoder) is trained to generate samples mimicing the samples produced by the prior (such that the discriminator tends to classify it as positive samples from prior), and the discriminator is trained to classify the samples from prior as positive and those from generator as negative. The optimal solution is that the generator generates samples exactly according to the prior distribution (i.e.~$q(\bb{h}) \propto r(\bb{z})$), and the discriminator classifies samples from either the prior or the generator as positive samples with equal probabilities (e.g.~$D(\bb{z}) = D(f_{enc}(\bb{x})) = 1/2$).



% ----------------------------------------------------
% ----------------------------------------------------

\section{Semantic Adversarial Autoencoder}

\subsection{Semantic Autoencoder}

For transfer learning tasks, the semantic autoencoder (SAE) \cite{kodirov2017semantic} is a tailor-made type of antoencoder-like structure that learns the transfer function between two domains. We consider the case in which one domain is the image feature $\bb{x}$, and the other domain is the attribute $\bb{t}$ (the ``semantic representation'') that describes the corresponding image. An autoencoder is trained to learn the transfer function $W$ through the optimization problem,
	\begin{align}
	\min_{W} || \bb{x} - W^{\top} W \bb{x} ||_2^2, \quad \text{s.t.} \;\; W \bb{x} = \bb{t}.
	\end{align}
	This model enforces the encoding of the image to be identical with the corresponding semantic representation. While this is a difficult constrained optimization problem, in the SAE architecture, the loss is relaxed to
	\begin{align}
	\label{eq:SAELoss} L_{\textrm{SAE}} = || \bb{x} - W^{\top} \bb{s}||_2^2 + \lambda || W \bb{x} - \bb{s} ||_2^2,
	\end{align}
	where a coefficient $\lambda$ is introduced to adjust the relative importance of the two terms. The SAE loss is a convex function, and in this form $W$ can be solved efficiently via the Bartels-Stewart algorithm \cite{lu1971solution}, which is adopted in \cite{kodirov2017semantic}.

	Note that the SAE is more like a domain transfer machine, rather than a standard autoencoder: It is not minimizing the reconstruction loss; it is minimizing the two directions of transfers between the two domains. Later in our experiments we find that in practice the learning may be majorly driven by one of the two directions (see section \ref{subsec:SAE}).



\subsection{Adversarial Autoencoder}

	In the adversarial autoencoder \cite{makhzani2015adversarial}, a generative adversarial net (GAN) \cite{Goodfellow2014} is incorporated into the autoencoder, and the prior in the GAN is chosen to regularize the hidden representation generated by the encoder. The positive samples $\bb{z}$ are drawn from the prior $r(\bb{z})$, while the generator $G$, which is simply the encoder in this architecture, generates negative samples $G(\bb{x})$ by encoding the input $\bb{x}$, which is drawn from the underlying data distribution $p_d(\bb{x})$. During training, the discriminator $D$ is trained to tell the positive samples from the negative samples, and the generator is trained to generate samples that has the aggregated distribution mimicking the prior distribution, so as to fool the discriminator. This procedure can be described as the optimization problem,
	\begin{align}
		\label{eq:AAE} \min_G \max_D \; E_{\bb{z} \sim r(\bb{z})}[ \log D(\bb{z}) ] + E_{\bb{x} \sim p_d(\bb{x})}[ \log( 1 - D(G(\bb{x})) ) + L_{\text{recon}} ],
	\end{align}
	where $L_{\text{recon}}$ is the reconstruction loss of the autoencoder. Through this procedure, the model learns to generate the encodings whose distribution is close to that of the prior by jointly minimizing the reconstruction loss from the autoencoder, the loss of misclassifying the positive and negative samples from the discriminator, and the loss of failing to generate positive samples from the generator.



% ----------------------------------------------------

\subsection{Semantic Adversarial Autoencoder}

	We propose the semantic adversarial autoencoder (SAAE) of which the generator learns to transfer the representation in one domain into that in another domain. The SAAE differs from the plain adversarial autoencoder in that it needs to encourage the input in one domain to be encoded into the representation in another domain (it learns a specific semantic). It also differs from SAE in that it uses the adversarial net to inject the global information of the distribution of the training data into the process of locally learning the encoding of each mini-batch of instances.

	We perform experiments on two architectures: the SAAE with explicit matching (SAAE-exp) and the SAAE with implicit matching (SAAE-imp). Their architectures are shown in Figure \ref{fig:Architecture}. In SAAE-exp, we explicitly require the encoding to approximate the semantic representation, and the positive samples is drawn from the Gaussian prior. In this case, the prior serves as a regularizer. In SAAE-imp, we directly use the semantic representation as the positive samples drawn from some underlying distribution that describes the semantic representation, and the encoder is trained by the adversarial net to match the semantic representation. In this case, the prior guides the generator (the encoder) to learn the encoding that is close to the semantic representation.


% ---------------------------------------------------------

\begin{figure*}[hbt!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.30in]{fig1}
        \caption{}
            \label{fig:Architecturea}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[height=2.30in]{fig2}
        \caption{}
                    \label{fig:Architectureb}
    \end{subfigure}
    \caption{Semantic adversarial autoencoders with (a) Gaussian prior and explicit matching to the text (or semantic/attribute) representation, and (b) text (semantic/attribute) representations as positive samples drawn from some underlying prior of the text (semantic/attribute) distribution.}
    \label{fig:Architecture}
\end{figure*}



% ----------------------------------------------------

\subsubsection{SAAE-exp}
\label{subsubsec:NoisePrior}

	SAAE-exp has an autoencoder that learns the encoding $\bb{h}^c$ of the input images $\bb{x}^c$ belonging to a class $c$. The encoding $\bb{h}^c$ is explicitly required to match the text (or, semantic/attribute)%
	\footnote{We interchangeably use ``text'', ``semantic'', and ``attribute'' in this report.} %
	representation $\bb{t}^c$ of that class, as well as regularized by the Gaussian prior in the adversarial net. The learning task can be expressed as the optimization problem,
	\begin{align}
	\min_{G} \max_D E_{\bb{z} \sim r(\bb{z})}\left[\; \log D(\bb{z}) \;\right] + E_{\bb{x} \sim p_d(\bb{x})}\left[\; \log( 1 - D(G(\bb{x})) ) + || G(\bb{x}) - \bb{t} ||_2^2 \;\right].
	\end{align}

	Given an input image representation $\bb{x} \in R^{d_x}$, the encoder encodes it into $\bb{h} \in R^{d_h}$ by
	\begin{align}
		\bb{h} = \tanh(W^e \bb{x}),
	\end{align}
	where $W^e \in R^{d_h \times d_x}$, and the hyperbolic tangent function is applied element-wisely. Note that the hidden representation space $R^{d_h}$ is also the space of the semantic representation; that is, $\bb{t} \in R^{d_h}$. The decoder computes
	\begin{align}
		\tilde{\bb{x}} = \tanh((W^e)^{\top} \bb{h}),
	\end{align}
	where $W^d \in R^{d_x \times d_h}$.
	
	The generator of the adversarial net is the encoder of the autoencoder; that is,
	\begin{align}
		G(\bb{x}) = \tanh(W^e \bb{x}) = \bb{h}.
	\end{align}
	The positive samples are drawn from the Gaussian prior,
	\begin{align}
		\bb{z} \sim \mathcal{N}(\mu, \sigma^2),
	\end{align}
	where $\bb{z} \in R^{d_h}$, and $\mu$ and $\sigma$ are chosen to be the mean and standard deviation of the semantic representation.
	
	We use a fully connected neural network with a single hidden layer as the discriminator. Given an input $\bb{z} \in R^{d_h}$, it computes
	\begin{align}
		\bb{z}^1 &= \textrm{sigmoid}( W^1 \bb{z} + \bb{b}^1 ), \\
		%
		z^2 &= \textrm{sigmoid}( (\bb{w}^2)^{\top} \bb{z}^1 + b^2 ),
	\end{align}
	where $\bb{z}^1 \in R^{d_1}$, $W^1 \in R^{d_1 \times d_h}$, $\bb{b}^1 \in R^{d_1}$, $\bb{w}^2 \in R^{d_1}$, and $b^2$ and $z^2$ are scalars. The discriminator returns
	\begin{align}
		D(\bb{z}) = z^2,
	\end{align}
	which is the estimation of the probability that $\bb{z}$ is drawn from the prior.
	
	For the autoencoder and the generator of the adversarial net, the loss function for a mini-batch of instances, $S$, is the empirical risk,
	\begin{align}
		f_g = \frac{1}{|S|} \sum_{i \in S} \left\{ \log\left[ 1 - D( \bb{h}^{(i)} ) \right] + || \bb{x}^{(i)} - \tilde{\bb{x}}^{(i)} ||_2^2 + || \bb{h}^{(i)} - \bb{t}^{c_i} ||_2^2 \right\},
	\end{align}
	where the log function is applied element-wisely, and $c_i$ is the class of the instance $\bb{x}^{(i)}$. Here we assume that there is only one text $\bb{t}^c$ for each class $c$.

	For the discriminator of the adversarial net, the loss function is
	\begin{align}
		f_d = -\frac{1}{|S|} \sum_{j = 1}^{|S|} \log D( \bb{z}^{(j)} ) - \frac{1}{|S|} \sum_{i \in S} \log\left[ 1 - D( \bb{h}^{(i)} ) \right],
	\end{align}
	where $\bb{z}^{(j)}$ are $|S|$ samples drawn from the prior distribution $\mathcal{N}(\mu, \sigma^2)$.
	
	The optimization problem for the autoencoder and the generator is
	\begin{align}
		\min_{W^e} \; f_g,
	\end{align}
	while the parameters of the discriminator ($W^1, \bb{b}^1, \bb{w}^2, b^2$) are held constant. The optimization problem for the discriminator is
	\begin{align}
		\min_{W^1, \bb{b}^1, \bb{w}^2, b^2} \; f_d,
	\end{align}
	while the parameters of the generator ($W^e, \bb{b}^e, W^d, \bb{b}^d$) are held constant.


% ----------------------------------------------------

\subsubsection{SAAE-imp}
\label{subsubsec:TextPrior}
	
	SAAE-imp takes the text representation $\bb{t}^c$ as the positive sample drawn from some underlying prior dictating the distribution of the text representation of a class. In this architecture, the encoding is driven to match the text representation through the adversarial net itself. The encoder, decoder, generator, and discriminator are the same as described in section \ref{subsubsec:NoisePrior}. The difference is that we use the text representation, instead of Gaussian noise, as the prior in the adversarial net. For an input image $\bb{x}^{(i)}$ of class $c_i$ and the text representation $\bb{t}^{c_i}$ of this class, we use $\bb{t}^{c_i}$ as the sample drawn from some underlying prior for the representation distribution of this class,
	\begin{align}
		\bb{z}^{c_i} = \bb{t}^{c_i}.
	\end{align}
	In addition, we remove the term measuring the L2 distance between the text representation and the hidden representation of the image from the loss function.

	The loss function for the autoencoder and the generator is now the standard one,
	\begin{align}
		f_g = \frac{1}{|S|} \sum_{i \in S} \left\{ \log\left[ 1 - D( \bb{h}^{(i)} ) \right] + || \bb{x}^{(i)} - \tilde{\bb{x}}^{(i)} ||_2^2 \right\}.
	\end{align}
	For the discriminator of the adversarial net, the loss function is
	\begin{align}
		f_d = -\frac{1}{|S|} \sum_{i \in S} \log D( \bb{t}^{c_i} ) - \frac{1}{|S|} \sum_{i \in S} \log\left[ 1 - D( \bb{h}^{(i)} ) \right].
	\end{align}
	
	The optimization and test procedures are the same as those described in section \ref{subsubsec:NoisePrior}.



% ----------------------------------------------------

\subsection{SAE-GAN}

	We also check whether adding GAN to the original architecture of SAE will change the performance of SAE. We solve the following optimization problem,
	\begin{align}
		\min_{G} \max_D E_{\bb{z} \sim r(\bb{z})}\left[\; \log D(\bb{z}) \;\right] + E_{\bb{x} \sim p_d(\bb{x})}\left[\; \log( 1 - D(G_{\textrm{SAE}}(\bb{x})) ) + L_{\textrm{SAE}} \;\right],
	\end{align}
	where
	\begin{align}
		G_{\textrm{SAE}}(\bb{x}) = W \bb{x}.
	\end{align}




% ----------------------------------------------------

\subsection{Classification task}

	There are two scenarios for ZSL, namely standard ZSL and generalized ZSL. For the standard ZSL, During training time, given training images $\bb{x}^c$ and their corresponding semantic representations $\bb{t}^c$ for $N_s$ seen classes $c$, the model learns to bridge the gap between $x^c$ and $t^c$. During testing time, we are given images $\bb{x}^{uc}$ from $N_{us}$ unseen classes and semantic representations $\bb{t}^{uc}$ for those unseen classes without knowing the correspondence between $\bb{x}^{uc}$ and $\bb{t}^{uc}$. The task is to determine which unseen class each image belongs to. For the generalized ZSL, the training procedure is the same, but the test images are not constrained to unseen classes: images from both seen and unseen classes can be used as test images and semantic representations for both classes are provided.

During the test time, we follow the same classification method as in \cite{kodirov2017semantic}. The input test image $\bb{x}$ is first projected to the semantic representation $\bb{h}$, and the class is predicted by
\begin{align}
	\hat{c} = \textrm{arg} \min_c \textrm{dist}(\bb{t}^c, \bb{h}),
\end{align}
where dist is the distance function. The distance function can be the negative cosine similarity, $\frac{(\bb{t}^c)^{\top} \bb{h}}{||\bb{t}^c||_2 ||\bb{h}||_2}$, or the Euclidean distance, $||\bb{t}^c - \bb{h}||_2$. We find that cosine similarity in general gives higher classification accuracy than the Euclidean distance does. We use the former in our experiments.


% ---------------------------------------------------------

\section{Datasets}

We conduct our experiments on two datasets. The first one is the Caltech-UCSD Birds 200-2011 dataset \cite{wah2011caltech}, with 200 categories of bird images. The total number of images is 11,788, and each class consists of about 40 to 80 images. The second dataset is AwA \cite{lampert2014attribute}, which consists of 30,475 images of 50 classes of animals.

Following the preprocessing steps taken in \cite{kodirov2017semantic}, for CUB dataset, we extract the 1024D activation from last pooling layer of Inception-v1 \cite{szegedy2015going} as our image features. For AwA dataset, we use the extracted 1024D features provided by \cite{kodirov2017semantic}. We use the attribute vector of each class as the semantic representation of the class. The dimensions of the attribute vectors are 312 and 85 for the CUB and AwA datasets, respectively.

For the standard ZSL tasks, following the description of \cite{kodirov2017semantic}, we withhold 10 and 50 classes as the unseen classes for AwA and CUB datasets, respectively. For the generalized ZSL tasks on the CUB dataset, we follow the description of \cite{Xian2017}%
\footnote{Since we do not have the information about the details of the splitting but only the number of instances in each split, we only follow roughly there number of instances used in each split, but not the exactly same split.}%
, using 7,125 training images from the 150 seen classes, and 4,663 test images from both 150 seen and 50 unseen classes. On the AwA dataset, we use 19,094 training images from the 40 seen classes, and 11,381 test images from both 40 seen and 10 unseen classes.

When we run our experiments, the image features and the attribute vectors are both normalized to $[0, 1]$ over the entire dataset.



% ---------------------------------------------------------

\section{Experiments}

We compare the results of our models with the Semantic Autoencoder (SAE) \cite{kodirov2017semantic}, the Joint Latent Semantic Embedding (JLSE) \cite{zhang2016zero}, and the Synthesized classifiers (Sync) \cite{changpinyo2016synthesized}. Due to the lack of an identical dataset, we decide to re-implement SAE to produce a reasonable baseline. It is worth noting that our implementation of SAE differs from that of \cite{kodirov2017semantic} in that we solve the optimization problem by gradient descent%
\footnote{We use the AdaGrad optimizer of TensorFlow.}%
in its original form, while in \cite{kodirov2017semantic} the problem is solved by first transforming it into a simpler linear equation. We find that the classification accuracy of the standard ZSL tasks using our implementation of SAE on both CUB and AwA datasets is lower than that reported in \cite{kodirov2017semantic} (see Table \ref{tab:StandardZSL}). Although we cannot reproduce the results in \cite{kodirov2017semantic}, we implement our models in the same framework which we use for implementing SAE, in the following discussion, we will therefore focus on comparing our results with our own implementation of SAE for self consistency. Table \ref{tab:StandardZSL} and \ref{tab:GeneralizedZSL} show the experimental results as well as the accuracy reported in \cite{kodirov2017semantic}.



\begin{table}[!htb]
\centering
\begin{tabular}{lrr}
\toprule
 & CUB & AwA \\
\midrule
SAAE-exp & 10.3 & 64.6 \\
SAAE-imp & 2.0 & 14.4 \\
SAE-dir & \bf{60.2} & 77.0 \\
SAE-GAN & 5.7 & \bf{78.5} \\
\midrule
SAE \cite{kodirov2017semantic} & 61.4 & 84.7 \\
JLSE \cite{zhang2016zero} & 41.8 & 80.5 \\
Sync \cite{changpinyo2016synthesized} & 54.4 & 72.9 \\
\bottomrule \\
\end{tabular}
\caption{Top-1 per-class accuracy (\%) of ZSL. SAAE-exp and SAAE-imp are the semantic adversarial autoencoder using Gaussian and attribute as priors, respectively. SAE-dir is our implementation of SAE which directly solve the optimization problem in its original form.}
\label{tab:StandardZSL}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{lrr}
\toprule
 & CUB & AwA \\
\midrule
SAAE-exp & \bf{17.8} & \bf{60.0} \\
SAAE-imp & 0.5 & 2.4 \\
SAE-dir & 7.0 & 53.8 \\
SAE-GAN & 6.9 & 54.0 \\
\bottomrule \\
\end{tabular}
\caption{Top-1 per-class accuracy (\%) of generalized ZSL. SAAE-exp and SAAE-imp are the semantic adversarial autoencoder using Gaussian and attribute as priors, respectively. SAE-dir is our implementation of SAE which directly solve the optimization problem in its original form.}
\label{tab:GeneralizedZSL}
\end{table}

In the experiments of SAAE with explicit matching, the coefficients of the matching, reconstruction, and GAN terms are all set to 1. We run for 100 epochs, and report the result top-1 per-class accuracy using negative cosine similarity as the distance at test time. The mean and standard deviation of the Gaussian priors are set to be the mean and standard deviation of the attributes of the dataset.

In the experiments of SAAE with implicit matching, the coefficient of the reconstruction term is 10 and that of the GAN is 1. We run for 100 epochs, and report the result top-1 per-class accuracy using negative cosine similarity as the distance at test time.

In the experiments of our implementation of SAE, we find that better performance is given when the coefficient of the matching loss is 100 and that of the reconstruction loss is 1. This indicates that the performance of SAE is mostly driven by the direct matching between the encoded representation from the image and the attribute vector, and is only marginally driven by the autoencoder.

%(wrong) the coefficients of the matching loss and the reconstruction loss are both set to 1. We run for 100 epochs, and report the result top-1 per-class accuracy using negative cosine similarity as the distance at test time.

We find that on the CUB dataset, SAAE with explicit matching achieves the best accuracy in both standard and generalized ZSL tasks. On the AwA dataset, our SAE implementation gives the best accuracy on both standard and generalized ZSL tasks.



% ----------------------------------------------

\section{Discussion}

% \subsection{SAE}

% In the experiment of our implementation of SAE, we find that the best results are achieved when the coefficients of the matching loss and the reconstruction loss are both set to 1 (compared to having much larger coefficient of the matching loss)


% One thing to notice is that the result of SAE is heavily affected by the $\lambda$ in the equation. If we set the $\lambda$ to be 1, the accuracy will stay around 5\% for AWA dataset. The result will only become close to the best result we have when the $\lambda$ is set to around 100000. After incorporating the adversarial part, the choice of $\lambda$ no longer matters: the accuracy will become close the the best result even we set the $\lambda$ to be 1. 



\subsection{SAAE-exp}

Our extension with adding adversarial part for the autoencoder does not yield higher accuracy than that achieved by the SAE in the standard ZSL scenario. We have experimented with a lot of other settings, such as adding more layers for the autoencoder, changing the hidden dimensions of the generator and the discriminator in the adversarial part, and different sampling strategies (such as sampling from the hidden state to produce negative samples, as described in \cite{makhzani2015adversarial}, section 2), but none of them gives any performance boost.

We suspect the reason is that the semantic features plays the essential role in the standard ZSL tasks. We also try ablation study that does not include the semantic space as constraints. In this case the model is just slightly better than random guess. This is expected since ZSL task usually deals with fine-grained image classification, and without the additional semantic space input, the models trained only based on image features are likely to have a poor result. However, the only way we figure out to incorporate the semantic features into the adversarial autoencoder is to minimize the L2 loss between the hidden state of the encoder and the semantic representation, and this major constraint limits the accuracy that can be achieved by the model.

% The second possible reason is that the prior is hard to pick for the semantic distribution. The prior serves to regularize the encoded samples so that the hidden state of an encoder is similar to the prior distribution we choose. In normal cases, for example alignDraw\cite{mansimov2015generating}, gaussian prior works fine. However, in out scenario, the output from encoder has to be close to the semantic space, thus our chosen prior has to resemble the distribution of the semantic space. As discussed before, the semantic space is the probability that one attribute appears in this class. If we view the attributes in the space of all possible classes, this is similar to a multinomial distribution. However, changing gaussian prior to multinomial distribution also does not work well.

The SAAE-exp model performs the best in generalized classification task. We think this is as expected due to the nature of the task. Adversarial autoencoder is proposed to inject the global information of the distribution of the semantic representation of the training data. Through testing the model on both the seen and unseen classes, the model performs well because it has learned the distribution over all the seen classes.

\subsection{SAAE-imp}

We observe that the classification accuracy using SAAE with the attribute as the prior is very low in all experiments. The reason is that using attribute vectors as the prior only encourages the encoding to have the same distribution as that of the attributes, but not educating the encoder to learn how to generate the encoding such that each component of the encoding can best match its corresponding component of the attribute vector. Since each component of the attribute vector has very specific meaning, and at test time we predict the class label by matching each component of the given attribute vector with the corresponding component of the encoding generated from the test image, lacking the ability to correctly predict each component of the attribute vector leads to the poor performance of this approach.


\subsection{SAE}
\label{subsec:SAE}

We note that SAE gives the best performance when the coefficient $\lambda$ in \eqref{eq:SAELoss} is much smaller than 1. This indicates that the learning is actually driven by learning how to generate (from attribute to reconstructed image), rather than how to infer (from image to attribute). We think this is an observation that has not be pointed out in the original SAE paper \cite{kodirov2017semantic}.


\subsection{SAE-GAN}

By adding GAN on top of the SAE, we find that on the AwA dataset it improves the accuracy, but on the CUB dataset it does not. For the standard ZSL on the AwA dataset, in which GAN makes the most significant improvement, we conduct more detailed experiments to check how accuracy changes by using different coefficients for the GAN. The results are shown in Table \ref{tab:SAEGAN}. We observe that with suitable coefficient, GAN slightly improves the accuracy, but if the coefficient of GAN is too large, it harms the performance.


\begin{table}[t]
\centering
\begin{tabular}{lccccc}
\toprule
GAN coefficient & $10^{-1}$ & $10^{-2}$ & $10^{-3}$ & $10^{-4}$ & 0 \\
Accuracy (\%) & 35.4 & 76.7 & 78.5 & 77.7 & 77.0 \\
\bottomrule \\
\end{tabular}
\caption{The top-1 per-class accuray of the standard ZSL on the AwA dataset, using different coefficients for the GAN in SAE-GAN.}
\label{tab:SAEGAN}
\end{table}


% ----------------------------------------------------
% ----------------------------------------------------

\section{Conclusion}

In this paper, we proposed to use the adversarial antoencoder framework incorporating semantic features as a solution to ZSL problem. Our method, the semantic adversarial autoencoder, projects the image representation into the semantic feature space with a discriminator matching the projection to a given prior. It outperforms the semantic autoencoder in the generalized zero-shot learning tasks on the AwA and CUB datasets.


\newpage
% ----------------------------------------------------
% ----------------------------------------------------

\bibliographystyle{plain}
\bibliography{MachineLearning}

\end{document}
